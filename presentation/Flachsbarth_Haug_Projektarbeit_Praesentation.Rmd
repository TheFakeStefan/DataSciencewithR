---
title: "Analyse der Bundestagswahlen 1990 - 2017"
subtitle: "inkl. Strukturdaten und Presseberichterstattung"
author: "<br/><br/>Stefan Flachsbarth, Martin Haug"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>

---
<div class="hdm"></div>

# Agenda


- Zielsetzung der Arbeit
- Aufbau der Projektarbeit
- Datenvorbereitung (Wahl, Struktur, Presse)
- Visualisierungen
  - Deskriptive & Explorative Statistik
  - Visualisierung mit Karten
- Text Mining
  - Text Mining
  - Sentiment Analyse
  - Visualisierung (Shiny/Elasticsearch)
- Resümee & Ausblick
- Manöverkritik
- Github Repository & Quellen

---
<div class="hdm"></div>

# Zielsetzung der Arbeit


- Aufbau auf Projektarbeit 'Programming for Data Science'
- Erweiterung: Analyse von Presseberichten
- Anwendung gelernter Inhalte
  - SQL-Datenbanken (PostgreSQL) und SQL-Queries
  - Datenaufbereiten in R
  - Funktionen und Iterationen in R
  - Unterschiede Python vs. R verstehen
  - Deskriptive und explorative Statistik
  - Nutzung von Join-Funktionen
  - Visualisierung mit Karten
  - Interaktiven Grafiken
- Exploration nicht explizit im Kurs behandelter Inhalte
- Interpretation der Ergebnisse

---
<div class="hdm"></div>

# Aufbau der Projektarbeit

** Struktur der Arbeit**
- Teil 1 : Laden und Aufbereiten der Wahldaten
- Teil 2 : Laden und Aufbereiten der Strukturdaten
- Teil 3 : Laden und Aufbereiten der Pressetexte
- Teil 4 : Visualisierungen Wahl-/Strukturdaten: Deskriptive & explorative Statistik
- Teil 5 : Visualisierung mit Karten
- Teil 6 : Text Mining Grundlagen
- Teil 7 : Text Mining Sentiment Analyse
- Teil 8 : Visualisierung mit shiny
- Teil 9 : Visualisierung mit Elasticsearch

**Datenquellen**
- Wahldaten: [Bundeswahlleiter](https://www.bundeswahlleiter.de/bundestagswahlen/2017/ergebnisse.html) 
- Strukturdaten: [Statistisches Bundesamt](https://www-genesis.destatis.de/gis/genView?GenMLURL=https://www-genesis.destatis.de/regatlas/AI-Z4-2011.xml&CONTEXT=REGATLAS01)
- Presseberichte: [WISO-Datenbank](https://www.wiso-net.de/dosearch/:3:ALLEQUELLEN-106_:3:PRESSE)

---
<div class="hdm"></div>

# Datenvorbereitung

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/Bundestag.jpg)

---
<div class="hdm"></div>

# Wahldaten

.pull-left[

**Datenvorbereitung**
- Daten im Excel-Format (einzelne Files pro Wahl)
- Ergänzen von zusätzlichen Spalten & berechneten Werten
- Anpassung von Datentypen
- Konsolidierung des Datensatzes
- Schreiben der Daten in SQL-Datenbank
]

.pull-right[
**Besonderheiten**
- Einsatz von **Funktionen vs. Iterationen**
- Umwandeln von Datentypen aufwendig (**string -> numeric**)
- Standardisierung mit Hilfe von Masterdaten (JOIN)
- Schreiben und Lesen von **Umlauten** problematisch
- **Varianten** von Funktionen (Veränderung des Parteienspektrums)
]

---
<div class="hdm"></div>

# Wahldaten



```{r, eval=FALSE}
# Wahldaten 2013 - Datenvorbereitung
df <- read_excel("Bundestagswahl_2013.xlsx")
df$Wahljahr <- "2013"
dfin_2013 <- df%>%
  cleanup01()%>%
  cleanup02()%>%
  cleanup03()%>%
  cleanup04()%>%
  mutate(Check = `UNIONPZ`+`SPDPZ` + `LINPZ` + `GRUPZ` + `FDPPZ` + `AFDPZ`)
head(dfin_2013,10)
```

---
<div class="hdm"></div>

# Strukturdaten

.pull-left[
**Datenvorbereitung**
- Daten im Excel-Format (einzelne Files pro Thema)
- Fehlende Werte unterschiedlich kodiert
- Umformungen und Umrechnungen sehr aufwendig
- Aggregationen entfernen
- Doppelte Daten entfernen
- Einzelne Werte berichtigen
- Schreiben der Daten in SQL-Datenbank
]

.pull-right[
**Besonderheiten**
- **Geschachtelte Funktionen**
- Umwandeln von Datentypen aufwendig (**string -> numeric**)
- Standardisierung über **Masterdaten (join)**
- Schreiben und Lesen von **Umlauten** problematisch
- Unterschiede **MacOS und Windows**
]

---
<div class="hdm"></div>

# Strukturdaten

```{r, eval=FALSE}
# Funktion, um die doppelt vorhandenen Landkreise zu eliminieren
altekreise <- function(stdf){
  stdf <- filter(stdf, Schluessel %notin% c("3152","14161","14166","14167", "..."))
  return(stdf) }

# Funktion, um die Strukturdaten mit den Stadt-/Landkreis-Masterdaten zu verknüpfen
lkrmaster <- function(stdf){
  stdf02 <- left_join(stdf, STLAID_Master, by=c("Schluessel" = "LKR_NR"))
  stdf02$'Stadt-/Landkreis' <- stdf02$'LKR_NAME'
  stdfin <- select(stdf02, -'Name', -'LKR_NAME', -'LAND_NR')
  return(stdfin) }

# Zusammenführen aller Teilfunktionen zu einer Gesamtfunktion
strukturprep <- function(stdf, length){
  stdfkrz <- transform02(stdf)
  stdfintro <- transform03(stdf)
  stdf <- transform04(stdfkrz,stdfintro)
  stdf <- transform05(stdf)
  stdf <- aggloeschen(stdf)
  stdf <- hamburg(stdf)
  stdf <- altekreise(stdf)
  stdf <- lkrmaster(stdf)
  return(stdf) }
```

---
<div class="hdm"></div>

# Presseberichte

.pull-left[
**Datenvorbereitung**
- Daten liegen im CSV & HTML-Format vor (Files mit je 50 Artikeln)
- Extraktion der Texte aus HTML Code
- Aufteilung des Textkorpus in Segmente (Headline, Autor, Text)
- Extraktion des Erscheinungsdatums aus komplexem Text
- Error Handling für fehlende Werte
- Schreiben der Daten in die SQL-Datenbank
]

.pull-right[
**Besonderheiten**
- Schreiben und Lesen von **Umlauten** problematisch
- Unterschiede **MacOS und Windows**
]

---
<div class="hdm"></div>

# Presseberichte

```{r, eval=FALSE}
# Definition einer Funktion, um die Daten in die SQL-Datenbank zu schreiben
WriteMyData <- function(table_name, table_content) {
  # Verbindung initialisieren
  con <- dbConnect(RPostgreSQL::PostgreSQL(),
    host = 'hdm-sql.think-data.de', 
    dbname = 'postgres',
    user = 'postgres',
    password = rstudioapi::askForPassword("Datenbank Password: ")
    )
  
  # Daten in die Datenbank schreiben
  dbWriteTable(con, table_name, as.data.frame(table_content), overwrite = TRUE)
  
  # Lesen der geschriebenen Daten
  return(dbReadTable(con, table_name))
  
  # Verbindung lösen
  dbDisconnect(con)
}
```

---

<div class="hdm"></div>

# PostgreSQL Datenbank


- **Wahldaten**: eine Tabelle pro Wahljahr

- **Strukturdaten**: eine Tabelle pro Thema (Daten mehrerer Jahre)

- **Presseberichte**: eine Tabelle pro Wahljahr

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/Image_wahldaten.PNG)


---

<div class="hdm"></div>

# Visualisierung

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/shinydashboards.PNG)

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)


**Beantwortung typischer politischer Fragestellungen**
- Welche Ergebnisse konnte die CSU bei der Bundestagswahl 2017 in den bayerischen Wahlkreisen erzielen?
- Welcher Kandidat hat im Wahlkreis Stuttgart das Direktmandat geholt?
- Wie haben sich die Ergebnisse der Parteien in Ostdeutschland im Zeitraum 1990- 2017 entwickelt?

**Vorgehensweise**
  - Datenextraktion
  - Visualisierung mit ggplot
  - Verschiedene Visualisierungsarten: Balken- und Liniendiagramm
  - Interpretation der Ergebnisse

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

**Welcher Kandidat hat im Wahlkreis Stuttgart das Direktmandat geholt?**

--

```{r, eval=FALSE}
# Zusammenführen der beiden Stuttgarter Wahlkreise in einen Datensatz
stuttgart <- bind_rows(stuttgart1, stuttgart2)

# Umstrukturierung der Datentabelle
stuttgart.long<-melt(stuttgart,id.vars="WKRNAME")

# Visualisierung der Ergebnisse in Form eines Barcharts
ggplot(stuttgart.long, aes(x=variable,y=value,fill=factor(WKRNAME)))+
  geom_bar(stat="identity",position="dodge")+
  scale_fill_discrete(name="WKRNAME",
                      breaks=c("Stuttgart I", "Stuttgart II"),
                      labels=c("Stuttgart I", "Stuttgart II"))+
  xlab("Partei")+ylab("Wählerstimmen")+
  theme_classic(base_size=7) +
  labs(title="Erstwählerstimmen in den beiden Stuttgarter Wahlkreisen")
```

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/stuttgartneu.PNG)

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

**Wie haben sich die Ergebnisse der Parteien in Ostdeutschland im Zeitraum 1990- 2017 entwickelt?**

--

```{r, eval=FALSE}
# Verbinden der einzelnen Datensätze aus den jeweiligen Wahljahren
ostwahl <- rbind(Ost2017, Ost2013, Ost2009, Ost2005, 
                 Ost2002, Ost1998, Ost1994, Ost1990)

ostwahl <- ostwahl%>%
  filter(WKRNAME=="Leipzig I")%>%
  select(Wahljahr, CDUPZ, SPDPZ, GRUPZ, LINPZ, AFDPZ, FDPPZ)

# Umwandlung des Datensatz zur Vorbereitung der Visualisierung
ostwahl.long <- melt(ostwahl,id.vars="Wahljahr")
ostwahl <- ostwahl.long

# Visualisierung der Daten mit Hilfe einen Liniendiagramms
ggplot(ostwahl, aes(x=Wahljahr, y=value, color=variable)) +
  geom_line() +
  theme_classic() +
  expand_limits(y=0)
```

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/ostwaehlerneu.PNG)


---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

**Entwicklung der Bruttoeinkommen in Deutschland - - - 1995 - 2017** 

--

```{r, eval=FALSE}
# Histogramm für 1995
plot1995 <- ggplot(bruttoeinkomm1995, aes(Bruttoentgelte)) +
  geom_histogram(bins = 50) +
  theme_classic() +
  labs(title="1995", x="Bruttoeinkommen in EUR", y="# LKR")

# Tabelle im Format 3 Spalten x 2 Zeilen
require(gridExtra)
grid.arrange(plot1995, plot2000, plot2005, plot2009, plot2013, plot2017, ncol=3)
```

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/Bruttohisto.PNG)

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

**Bruttoeinkommen 1995-2017 - - - München vs. Sachsen-Anhalt** 

--

```{r, eval=FALSE}
# Datenselektion und -aufbereitung für Sachsen-Anhalt.
bruttoeinkomm_ST <- bruttoeinkomm%>%
  filter(LAND_ABK=="ST")%>%
  rename("Bruttoentgelte"="Bruttoentgelte.je.Beschäftigten.in.Tsd..EUR")%>%
  filter(Bruttoentgelte != "NA")%>%
  select(-Column1)
bruttoeinkomm_ST <- within(bruttoeinkomm_ST, {
        Bruttoentgelte <- as.numeric(as.character(Bruttoentgelte))
})
bruttoeinkomm_ST <- bruttoeinkomm_ST%>%
  group_by(Year)%>%
  summarize(median(Bruttoentgelte, Year))%>%
  mutate(Standort = "Sachsen-Anhalt")%>%
  rename("Bruttoentgelte"="median(Bruttoentgelte, Year)")
# Visualisierung mit Hilfe eines Liniendiagramms
ggplot(bruttoeinkommen_M_ST, aes(x=Year, y=Bruttoentgelte, color=Standort)) +
  geom_line() +
  theme_classic() +
  expand_limits(y=0) +
  labs(title="Bruttoeinkommen 1995 - 2017", x="Jahr", y="Bruttoentgelte")
```

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/Muenchen_SachsenAnhalt.PNG)

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

**Verfügbare Einkommen - - - Verteilung im Zeitverlauf** 

--

```{r, eval=FALSE}
# Daten für 2002 aufbereiten
verfeinkomm2002 <- verfeinkomm%>%
  filter(Year=="2002")%>%
  rename("Verf_Einkommen"="Verfügbares.Einkommen.je.Einwohner.in.EUR")%>%
  filter(Verf_Einkommen != "NA")%>%
  select(-Column1)%>%
  mutate(OstWest = ifelse(LAND_ABK %in% c("MV","BB","SN","TH","ST"),"Ost","West"))
verfeinkomm2002 <- within(verfeinkomm2002, {
        Verf_Einkommen <- as.numeric(as.character(Verf_Einkommen))
})

# Boxplot für 2016
plot2016 <- ggplot(verfeinkomm2016, aes(x=OstWest, y=Verf_Einkommen, fill=OstWest)) +
  geom_boxplot(outlier.size=2) +
  theme_light() +
  scale_y_log10() +
  labs(title="Verf. Einkommen (Ost/West) - 2016", x="", y="")

# Anordnen der Boxplots in Tabellenform
require(gridExtra)
grid.arrange(plot2002, plot2009, plot2013, plot2016, ncol=2)
```

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/VerfuegbaresEinkBox.PNG)

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)



**Analysen**
- Verteilung 'Bruttoeinkommen' & 'Verfügbares Einkommen'
- Vergleich Ost-West
- Trends im Zeitverlauf
- Hypothesentest

**Visualisierung mit ggplot und beeswarm**


---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

**Verfügbares Einkommen in Ost und West - - - 2002 - 2016** 

--

```{r, eval=FALSE}
# Verbindung aller Daten in einem Datensatz 
verfeinkommen_allbee <- rbind(verfeinkomm2002, verfeinkomm2005, 
                              verfeinkomm2009, verfeinkomm2013, verfeinkomm2016)

head(verfeinkommen_allbee, 5)

# Visualisierung mit Hilfe eines 'Beeswarm-Charts'
library(ggbeeswarm)
ggplot(verfeinkommen_allbee, aes(x=Year, y=Verf_Einkommen, color=OstWest)) +
  geom_beeswarm(dodge.width=0.1) +
  labs(title="Verfügbare Einkommen (Ost/West)", 
       subtitle="Zeitraum 2002 - 2016", 
       x="Jahr", 
       y="Verfügbares Einkommen in EUR")
```

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/VerfuegbarBeeswarm.PNG)

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

**Bruttoeinkommen in Deutschland - - - 1995 - 2017** 

--

```{r, eval=FALSE}
# Vorbereitung der Daten
bruttoeinkommen <- bruttoeinkomm%>%
  rename("Bruttoentgelte"="Bruttoentgelte.je.Beschäftigten.in.Tsd..EUR")%>%
  filter(Bruttoentgelte != "NA")%>%
  select(-Column1)%>%
  mutate(OstWest = ifelse(LAND_ABK %in% c("MV","BB","SN","TH","ST"),"Ost","West"))
bruttoeinkommen <- within(bruttoeinkommen, {
        Bruttoentgelte <- as.numeric(as.character(Bruttoentgelte))
})

head(bruttoeinkommen, 5)

# Visualisierung mit Hilfe eines 'Beeswarm-Charts'
ggplot(bruttoeinkommen, aes(x=Year, y=Bruttoentgelte, color=OstWest)) +
  geom_beeswarm(dodge.width=0.1) +
  labs(title="Bruttoeinkommen (Ost/West)", 
       subtitle="Zeitraum 1995 - 2017", 
       x="Jahr",
       y="Bruttoeinkommen in EUR")
```

---

<div class="hdm"></div>

# Visualisierung (Wahl-/Strukturdaten)

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/BruttoBeeswarm.PNG)

---


<div class="hdm"></div>

# Hypothesentest


Die **Hypothese ist, dass sowohl im Jahr 1995 als auch im Jahr 2017 die statistischen Unterschiede bei den Bruttoeinkommen in Ost und West nach wie vor statistisch signifikant sind**. Mathematisch kann man das wie folgt ausdrücken:

*H1a: mean_bruttoeinkommen(West-1995) - mean_bruttoeinkommen(Ost-1995) > 0*
<br/> *H1b: mean_bruttoeinkommen(West-2017) - mean_bruttoeinkommen(Ost-2017) > 0*

Die **Null-Hypothese** ist folglich die Aussage, dass es weder 1995 noch 2017 statistisch signifikante Unterschiede bei den Bruttoeinkommen in Ost und West gab. Mathematisch gesprochen:

*H1a: mean_bruttoeinkommen(West-1995) - mean_bruttoeinkommen(Ost-1995) <= 0* 
<br/>*H1b: mean_bruttoeinkommen(West-2017) - mean_bruttoeinkommen(Ost-2017) <= 0*

Angelegt werden soll bei unserem Test ein **Signifikanzniveau von 0.01**, um die Nullhypothesen zu verwerfen. Werden **beide Varianten der Nullhypothese verworfen, wird die Gesamthypothese als bestätigt gewertet**.

---

<div class="hdm"></div>

# Hypothesentest

```{r, eval=FALSE}
# t-Test für Daten des Jahres 1995
X1 <- as.vector(bruttoeinkommen1995west$Bruttoentgelte)
Y1 <- as.vector(bruttoeinkommen1995ost$Bruttoentgelte)

t.test(x=X1, y=Y1, alternative = c("two.sided","less",
 "greater"), mu=0, var.equal=F, paired=F, conf.level=0.99)

# t-Test für Daten des Jahres 2017
X1 <- as.vector(bruttoeinkommen2017west$Bruttoentgelte)
Y1 <- as.vector(bruttoeinkommen2017ost$Bruttoentgelte)

t.test(x=X1, y=Y1, alternative = c("two.sided","less",
 "greater"), mu=0, var.equal=F, paired=F, conf.level=0.99)
```

---

<div class="hdm"></div>

# Hypothesentest

**Interpretation**

Zunächst werden die **Daten des Jahres 1995** betrachtet. Die Ergebnisse des t-Tests zeigen, dass die Nullhypothese H1a verworfen werden kann. Der **t-Wert ist mit 21,3 deutlich über dem Signifikanzniveau von 10,4**, welches bei einem Konfidenzniveau von 99% bei einem zweiseitigen t-Test angelegt wird. Der **p-Wert von annähernd null** zeigt, dass der Unterschied zwischen den ost- und westdeutschen Landkreisen auch statistisch signifikant ist.

Im zweiten Schritt betrachten wir uns die **Daten des Jahres 2017**. Die Ergebnisse des t-Tests weisen hier einen **t-Wert von 15,5 auf, was ebenfalls oberhalb des Signifikanzniveaus von 14,0** liegt. Der **p-Wert ist auch hier annähernd null**, so dass auch in diesem Fall der Unterschied zwischen den ost- und westdeutschen Landkreisen statistisch signifikant ist. Wir können also auch die Nullhypothese H1b verwerfen.

Abschließend können wir zusammenfassen, dass die **Gesamthypothese als bestätigt betrachtet werden kann**. Grund: **Sowohl die Nullhypothese H1a für das Jahr 1995 als auch die Nullhypothese H1b für das Jahr 2017 konnten verworfen werden**. Die Unterschiede zwischen beiden Gruppen waren in der Vergangenheit und sind auch heute (i.e. 2017) noch statistisch signifikant.

---

<div class="hdm"></div>

# Visualisierungen auf Karten

- Strukturdaten auf **Landkreis-Ebene**
- Kartenmaterial und Wahlergebnisse auf **Wahlkreis-Ebene**
- Manuelle Verknüpfung der Land- und Wahlkreise

--

```{r, eval=FALSE}
# Einlesen der Karte
shapefile <- readOGR("deutschland.shp")

# Einlesen der Strukturdaten
beschaeftigung <- ReadData("beschaeftigung.csv")
beschaeftigung_2017 <- beschaeftigung %>% 
  filter(jahr == 2017) %>% rename(LKR_NR = X) %>% 
  mutate(LKR_NR = as.numeric(LKR_NR))

# Zusammenführen der Strukturdaten mit dem Mapping
beschaeftigung_2017 <- merge(x=beschaeftigung_2017, y=mapping_df, 
                             by.x="LKR_NR", by.y="LKR_NR") %>% 
  subset("LKR_NR", "WKR_NR", "beschaftigungsquote")

# Zusammenführen der Karte und Daten
map <- merge(x=germany, y = beschaeftigung_2017, by.x="WKR_NR", by.y="WKR_NR" )

# Ausgabe
plot(map['beschaftigungsquote'], main = "Beschäftigungsquote von 2017")
```

---
<div class="hdm"></div>

# Visualisierungen auf Karten

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/Maps.png)

---

<div class="hdm"></div>

# Text Mining

- Extraktion der Informationen aus HTML-Datei
- Zerlegen der Artikeltexte und -überschriften
- Vereinheitlichung der Umlaute
- Stopword-Filtering und Stemming

--

```{r, eval=FALSE}
# Liste mit deutschen Stoppwörtern
stopword <- as_tibble(stopwords::stopwords("de")) 

# Erweiterte Liste mit deutschen Stoppwörtern
# https://githubt.com/solariz/german_stopwords
stopword_extented <- read_tsv("german_stopwords_full.txt", comment = ";")

# Liste der eigenen Stoppwörtern
stopword_own <- tibble(word = c("bz", "mz", "rp", "sz", "tz", "ta", "taz"))

# Zusammenfügen und entfernen von Duplikaten
all_stopword <- bind_rows(stopword, stopword_extented, stopword_own) %>% distinct()

tb_text <- presse %>% unnest_tokens(token, text, token = "words", 
                                    format = "text", to_lower = TRUE, drop = TRUE)

tb_header <- anti_join(token_header, all_stopword, by = 'word')
tb_text <- anti_join(token_text, all_stopword, by = 'word')

```
---
<div class="hdm"></div>

# Explorative Datenanalyse

- Worthäufigkeiten nach:
  - Artikeltexten
  - Publikation
  - Parteien

```{r, eval=FALSE}
# Durchsuchen der Artikel nach eines Partei
article_patei_linke_count <- tb_text %>% 
  filter(word == "linken" | word == "pds") %>% 
  group_by(year) %>% 
  count(year, sort = TRUE) %>% ungroup()
article_patei_linke_count$word <- "linke"

# Zusammenführen der Daten
article_patei_total <- rbind(article_patei_cdu_count, article_patei_spd_count,
                             article_patei_fdp_count, article_patei_gruene_count,
                             article_patei_linke_count, article_patei_afd_count)

# Kalkulation der prozentualen Anteile
article_patei_percent <- group_by(article_patei_total, year) %>% 
                         mutate(percent = n/sum(n) * 100) %>%
                         ungroup()
```

---
<div class="hdm"></div>

# Explorative Datenanalyse

- Worthäufigkeiten nach:
  - Artikeltexten
  - Publikation
  - Parteien

```{r, eval=FALSE}
# Zuweisen der Farben
pateifarben <- c("blue", "black", "yellow", "green", "purple", "red")

# Ausgeben des Balkendiagramms
ggplot(data=article_patei_percent, 
       aes(x=year, y=percent, fill=word)) +
  geom_bar(stat="identity",
           position='stack')+
  scale_fill_manual(values=pateifarben) +
  xlab("Nennungen in der Presse") +
  ylab("Prozent") +
  theme_minimal() +
  theme(text = element_text(size=20),
        plot.background = element_rect(fill = "#FAFAFA"),
        legend.title = element_blank()) +
  ggtitle("Relevanz der Partei in der Presse")
```

---
<div class="hdm"></div>

# Explorative Datenanalyse

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/Worthaeufigkeiten_nach_Parteien.png)

---
<div class="hdm"></div>

# Explorative Datenanalyse

**Vergleich von Pressepräsenz und realen Wahlergebnissen**

```{r, eval=FALSE}
# Funktion, um die Wahlergebnisse der jeweiligen Partei einzulesen
read_wahlergebnis <- function(partei) {
  z <- read_csv2("bundestagswahlergebnisse_1990_2017.csv", 
                 col_type = cols()) %>% 
  select(Wahljahr,toupper(partei)) %>% 
  as.data.frame() %>%
  rename("year" = Wahljahr,"percent" = toupper(partei))
  z$type <- "r" # Reales Ergebnis
  z$word <- toupper(partei)
  return(z)
}

# Anwenden der Funktion auf die Partei CDU
wahlergbnis_cdu <- read_wahlergebnis("cdu")

# Markieren der Gruppe der Daten
article_patei_percent$type <- "p" # Presse Ergebnis

# Zusammenführen mit den Pressedaten
graphdata <- rbind(article_patei_percent, wahlergbnis_patei_percent)
```

---
<div class="hdm"></div>

# Explorative Datenanalyse

**Vergleich von Pressepräsenz und realen Wahlergebnissen**

```{r, eval=FALSE}
# Richtige Farbkennung der Parteien
pateifarben <- c("blue", "black", "yellow", "green", "purple", "grey", "red")

# Plotten der Grafik
ggplot(data=graphdata, aes(x=type, y=percent, fill=word )) +
  geom_bar(stat="identity",
           position='stack')+
  facet_grid( ~ year) +
  scale_fill_manual(values=pateifarben) +
  xlab("(P)resse VS (R)ealität") +
  ylab("Prozent") +
  labs(caption = "... auf Basis der Nennungen in den Artikeltexten.") +
  theme_minimal() +
  theme(text = element_text(size=20),
        plot.background = element_rect(fill = "#FAFAFA"),
        legend.title = element_blank()) +
  ggtitle("Vergleich der Wahlergebnisse mit der Relevanz in der Presse")
```

---
<div class="hdm"></div>

# Explorative Datenanalyse

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/Vergleich_Wahlergbnis_Presse.png)
<div class="hdm"></div>

---
<div class="hdm"></div>

# Sentimentanalyse

- **Analyse und Vergleich des Sentiments**
- **Lexikon der Universität Leipzig** als Grundlage

```{r, eval=FALSE}
# Einlesen des Lexikons
negative_worte <- read_tsv("sentiment_lexikon.txt", col_names = FALSE)

# Umbenennen der Spalten
names(negative_worte) <- c("Wort_POS", "Wert", "Inflektionen")

# Negative Wörter und Beugungen dieser Extrahieren
negative_worte <- negative_worte %>% 
  mutate(Wort = str_sub(Wort_POS, 1, regexpr("\\|", .$Wort_POS)-1),
         POS = str_sub(Wort_POS, start = regexpr("\\|", .$Wort_POS)+1))

# Zusammenführen der postitiven und negativen Wörter
sentiment_df <- bind_rows("neg" = negative_worte,
                          "pos" = positive_worte, .id = "neg_pos")

# Selektieren der relevanten Spalten
sentiment_df <- select(sentiment_df, neg_pos, Wort, Wert, Inflektionen, -Wort_POS)
```

---
<div class="hdm"></div>

# Sentimentanalyse

Vergleich einer Ost- und Westpublikation (**Sächsische Zeitung** vs. **Badische Zeitung**)

```{r, eval=FALSE}
# Selektion der Badischen Zeitung 
tb_text_basische_zeitung <- filter(tb_text, publisher == "badische zeitung") 

# Berechnung des Scores für die negativen Wörter 
tb_text_basische_zeitung_sentiment_neg <- 
  match(tb_text_basische_zeitung$word, filter(sentiment_df, neg_pos == "neg")$word)
tb_text_basische_zeitung_neg_score <- 
  sum(!is.na(tb_text_basische_zeitung_sentiment_neg))

# Berechnung des Scores für die positiven Wörter 
tb_text_basische_zeitung_sentiment_pos <- 
  match(tb_text_basische_zeitung$word, filter(sentiment_df, neg_pos == "pos")$word)
tb_text_basische_zeitung_pos_score <- 
  sum(!is.na(tb_text_basische_zeitung_sentiment_pos))

# Berechnung des Ergebnisses
round(tb_text_basische_zeitung_pos_score/tb_text_basische_zeitung_neg_score, 1)
```

--

.center[
    Badische Zeitung    |    Sächsische Zeitung  
----------------------- | -----------------------
          2.0           |           2.5  
]

---
<div class="hdm"></div>

# Sentimentanalyse

**Vergleich des Sentiments über die vergangenen Wahljahre**

```{r, eval=FALSE}
# Funktion zur Berechnug des Sentiments pro Wahljahr 
getSentiment <- function(text,jahr) {
  x <- text %>% filter(year == jahr) 
  
  x_senti_neg <- match(x$word, filter(sentiment_df, neg_pos == "neg")$word)
  x_neg_score <- sum(!is.na(x_senti_neg))
  
  x_senti_pos <- match(x$word, filter(sentiment_df, neg_pos == "pos")$word)
  x_pos_score <- sum(!is.na(x_senti_pos))
  # Abfangen möglicher Nullwerte von x_neg_score
  senti_sum <- ifelse(is.infinite(x_pos_score / x_neg_score),
              x_pos_score,x_pos_score / x_neg_score) %>% as.numeric()
  print(paste("Der Sentiment für das Jahr", jahr, "beträgt", y))
  return(tibble(y,jahr))
}

senti_header_1990 <- getSentiment(tb_header, "1990")
## [1] "Der Sentiment für das Jahr 1990 beträgt 5"
senti_header_1990 <- getSentiment(tb_text, "1990")
## [1] "Der Sentiment für das Jahr 1990 beträgt 1.38461538461538"
```

---
<div class="hdm"></div>

# Sentimentanalyse

**Vergleich des Sentiments über die vergangenen Wahljahre**

```{r, eval=FALSE}
# Zusammenführen der Dataframes
senti_header_all$src <- "Titel"
senti_text_all$src <- "Text"
senti_all <- rbind(senti_header_all, senti_text_all) 

# Plotten der Grafik
ggplot(data=senti_all, aes(x = jahr, y = senti_sum, fill = src )) +
  geom_bar(stat="identity",
          position=position_dodge())+
  xlab("Jahr") +
  ylab("Sentiment") +
  theme_minimal() +
  theme(text = element_text(size=20),
        plot.background = element_rect(fill = "#FAFAFA"),
        legend.title = element_blank()) +
  ggtitle("Vergleich der Sentimente über die vergangenen Wahljahre")
```

---
<div class="hdm"></div>

# Sentimentanalyse

![](https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/Sentimente_ueber_Wahljahre.png)

---

<div class="hdm"></div>

# Shiny

**Features der Applikation**
- 2 Datenquellen (Artikelüberschrift und -text)
- Dynamisches Minimum und Maximum der Token
- Deployment über ShinyApps - [http://hdm-shiny.think-data.de/](http://hdm-shiny.think-data.de/)
  
**Unzureichende Performance**
- Reduzierung der Tokenlisten
- Caching der Token

--

```{r, eval=FALSE}
top_ueberschrift_liste <- artikel_df %>% count(word) %>% top_n(500)
top_text_liste <- artikel_df %>% count(word) %>% top_n(500)

build_wordcloud <- memoise(function(quelle) {
  if (quelle == "Artikeltext")
  { text <- top_text_liste }
  if (quelle == "Artikelüberschrift") 
  { text <- top_ueberschrift_liste }
  # Erstellen des Objektes für die Wordcloud
  [...] })
```

---

<div class="hdm"></div>

# Elasticsearch und Kibana

Importieren der Textdaten in Elasticsearch

Aufzeigen von Zusammenhängen zwischen:
- Wörtern in Artikeln
- Wörtern und Publikationen
- Wahlkampfthemen der vergangenen Jahre

Explorative Analyse

```{r, eval=FALSE}
klima_suche <- query('{
    "multi_match": {
      "query": "(klima) OR (klimakrise) OR (umwelt)",
      "fields": [ "header", "text" ]
    }}')

klima <- elastic("https://user:password@elasticsearch:9200", 
                 "presse*") %search% (klima_suche)
```

Live Demonstration unter [http://hdm-kibana.think-data.de/](http://hdm-kibana.think-data.de/) 

---

<div class="hdm"></div>

# Resümee und Ausblick

**Resümee** - Sowohl Python als auch R für Aufgabenstellung geeignet

.pull-left[
Vorteile von R
- Datenaufbereitung einfacher, da R bspw. für Dataframes optimiert
- Höherer Reifegrad - reichhaltiges Angebot an Paketen
]

.pull-right[
Nachteile von R
- Vielzahl von Paketen führt zu Verwirrung
- Schlechte Dokumentation
]

--

**Ausblick - "Wenn mehr Zeit gewesen wäre..."**
- Weitere *statistische Auswertungen*
- Datenbestand bietet vielfältige Möglichkeiten für *Korrelationsanalysen*
- Einsatz von *Machine Learning Algorithmen*, z.B. für Klassifizierung, Clustering
- Weiterführung der *Textanalyse*
- Austesten weiterer *interaktiver Visualisierungen*
- **Code-Reduktion durch Einsatz von Iterationen, kürzeren Befehlen, etc.**

---

<div class="hdm"></div>

# Manöverkritik

.pull-left[
**Highlights**
- Interessante Datenquellen und Fragestellungen
- Datenaufbereitung in R sehr komfortabel
- Vielfalt an Visualisierungstool
- Großes Experimentierfeld
]

.pull-right[
**Lowlights**
- Zeit für Projektarbeit im Grunde genommen zu kurz
- Vor allem Umfang und Eleganz des Codes haben dadurch gelitten
- 'Unübersichtlichkeit' von R im Vergleich zu Python
- Überblick bekommen und Struktur verstehen für Anfänger schwierig
- R hinterlässt 'fragmentarischen' Eindruck
- Erfolgreiches 'Run All' != erfolgreiches Knitting
- Nicht alles, was auf MacOS funktioniert, klappt auch in Windows
- Umgang mit deutschen Umlauten und Sonderzeichen sehr knifflig
]

---

<div class="hdm"></div>

# GitHub - Repository

- **Datenquellen**
  - Wahldaten: [Bundeswahlleiter](https://www.bundeswahlleiter.de/bundestagswahlen/2017/ergebnisse.html) 
  - Strukturdaten: [Statistisches Bundesamt](https://www-genesis.destatis.de/gis/genView?GenMLURL=https://www-genesis.destatis.de/regatlas/AI-Z4-2011.xml&CONTEXT=REGATLAS01)
  - Presseberichte: [WISO-Datenbank](https://www.wiso-net.de/dosearch/:3:ALLEQUELLEN-106_:3:PRESSE)

- **Präsentation**
  - Xaringan-Code
  - Bilder: [Deutscher Bundestag](http://bilderdienst.bundestag.de/journals/public_collections.php), [shiny](https://shiny.rstudio.com/)
  - Daten

- **Anregungen zum Code**
  - Recherche zu **aktuellen** Paketen und Funktionen
  - Verwendung eines gemeinsamer Codebasis

- **Github-Repository**
  - Zugang: [http://hdm-git.think-data.de/](https://github.com/TheFakeStefan/DataSciencewithR)

---
class: inverse, middle, center

<div class="hdm"></div>

# Danke für Eure Aufmerksamkeit.

<style>
div.hdm {
  background-image: url(https://raw.githubusercontent.com/TheFakeStefan/DataSciencewithR/master/images/presentation/hdm.png);
  background-position: 90% 1.3%;
  background-size: 70px;
  position: fixed;
  top: 1.3%;
  left: 90%;
  height: 62.5px;
  width: 70px;
}

.remark-slide-number {
  position: inherit;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 5px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: #23373B;
}

</style>

